{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP(Multi_Layer_Perceptron).ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPkpPJUisgNvGOh3ZJziBGF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raiakash203/Machine-Learning/blob/Dimension-Reduction/MLP(Multi_Layer_Perceptron).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4xHGfxAuR2z",
        "colab_type": "text"
      },
      "source": [
        "## MLP Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxrQfcSqpmrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==1.14"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQKeU35pqlem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist=input_data.read_data_sets(\"MNIST\", one_hot=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ngCxq7hucle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe0Ei2VeumSX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_dynamics(x,y,y_1,ax,ticks,title,colors=['b']):\n",
        "  ax.plot(x,y,'b',label='Train Loss')\n",
        "  ax.plot(x,y_1,'r',label='Test Loss')\n",
        "  if len(x)==1:\n",
        "    plt.legend()\n",
        "    plt.title(title)\n",
        "  plt.yticks(ticks)\n",
        "  fig.canvas.draw()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kqAtX2Tu9qs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Network Parameter\n",
        "n_hidden_1 = 512  # 1st layer neurons\n",
        "n_hidden_2 = 128 # 2nd Layer neurons\n",
        "n_input = 784 #MNIST data inputs\n",
        "n_classes = 10 #MNIST output labels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guWTmINdvvr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.placeholder(tf.float32,[None,784])\n",
        "y_ = tf.placeholder(tf.float32,[None,10])\n",
        "\n",
        "#These are used when using dropouts\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "keep_prob_inputs = tf.placeholder(tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cheKiVvRwl33",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Weights initialization\n",
        "\n",
        "#When using normal sigmoid activation unit we use Xavier/Gloro weight initialization based on fan_in adn fan_out\n",
        "#For h1 fan_in=784, fan_out = 512\n",
        "#For h2 fan_in=512 fan_out = 128\n",
        "#and so on\n",
        "\n",
        "# h1 => σ=√(2/(fan_in+fan_out+1) = 0.039 => N(0,σ) = N(0,0.039)\n",
        "# h2 => σ=√(2/(fan_in+fan_out+1) = 0.055 => N(0,σ) = N(0,0.055)\n",
        "# out => σ=√(2/(fan_in+fan_out+1) = 0.120 => N(0,σ) = N(0,0.120)\n",
        "\n",
        "weights_sgd = {\n",
        "    'h1' : tf.Variable(tf.random.normal([n_input,n_hidden_1],stddev=0.039,mean=0)),\n",
        "    'h2' : tf.Variable(tf.random.normal([n_hidden_1,n_hidden_2],stddev=0.055,mean=0)),\n",
        "    'out' : tf.Variable(tf.random.normal([n_hidden_2,n_classes],stddev=0.120,mean=0))\n",
        "\n",
        "}\n",
        "\n",
        "#In case of ReLu activation function, we generally use He initialization, for normal\n",
        "# h1 => σ=√(2/(fan_in+1) = 0.062 => N(0,σ) = N(0,0.062)\n",
        "# h2 => σ=√(2/(fan_in+1) = 0.125 => N(0,σ) = N(0,0.125)\n",
        "# out => σ=√(2/(fan_in+1) = 0.120 => N(0,σ) = N(0,0.120)\n",
        "\n",
        "weights_relu = {\n",
        "    'h1' : tf.Variable(tf.random.normal([n_input,n_hidden_1],stddev=0.062,mean=0)),\n",
        "    'h2' : tf.Variable(tf.random.normal([n_hidden_1,n_hidden_2],stddev=0.125,mean=0)),\n",
        "    'out' : tf.Variable(tf.random.normal([n_hidden_2,n_classes],stddev=0.120,mean=0))\n",
        "\n",
        "}\n",
        "\n",
        "#Bias initialization\n",
        "\n",
        "biases = {\n",
        "    'b1' : tf.Variable(tf.random.normal([n_hidden_1])),\n",
        "    'b2' : tf.Variable(tf.random.normal([n_hidden_2])),\n",
        "    'out' : tf.Variable(tf.random.normal([n_classes]))\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wJbQs_HzmZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters\n",
        "train_epooch = 15\n",
        "learning_rate = 0.001\n",
        "batch_size = 100\n",
        "display_step = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwsZZkxy0ASw",
        "colab_type": "text"
      },
      "source": [
        "# Model1 : input(784) - sigmoid(512) - sigmoid(128) - softmax(10)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz9fVaD6z1ND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create Model\n",
        "\n",
        "def multilayer_perceptron(x,weights,biases):\n",
        "  print('X: ',x.get_shape(),'W[h1] : ',weights['h1'].get_shape(),'b[h1]: ',biases['b1'].get_shape())\n",
        "\n",
        "  #Hidden layer with sigmoid activation\n",
        "  layer_1 = tf.add(tf.matmul(x,weights['h1']),biases['b1'])\n",
        "  layer_1 = tf.nn.sigmoid(layer_1)\n",
        "  print('layer_1: ',layer_1.get_shape(),'W[h2] : ',weights['h2'].get_shape(),'b[h2]: ',biases['b2'].get_shape())\n",
        "\n",
        "  #Hidden layer with sigmoid activation\n",
        "  layer_2 = tf.add(tf.matmul(layer_1,weights['h2']),biases['b2'])\n",
        "  layer_2 = tf.nn.sigmoid(layer_2)\n",
        "  print('layer_2: ',layer_2.get_shape(),'W[out] : ',weights['out'].get_shape(),'b[out]: ',biases['out'].get_shape())\n",
        "  \n",
        "  #Output layer with sigmoid activation\n",
        "  out_layer = tf.add(tf.matmul(layer_2,weights['out']),biases['out'])\n",
        "  out_layer = tf.nn.sigmoid(out_layer)\n",
        "  print('out_layer: ',out_layer.get_shape())\n",
        "\n",
        "  return out_layer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpLyUvmu3pb4",
        "colab_type": "text"
      },
      "source": [
        "## Model1 + AdamOptimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAm0LL1r3ofW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_sgd = multilayer_perceptron(x,weights_sgd,biases)\n",
        "\n",
        "cost_sgd = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_sgd, labels=y_))\n",
        "\n",
        "optimizer_adam = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_sgd)\n",
        "optimizer_sgdc = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost_sgd)\n",
        "\n",
        "with tf.Session() as session:\n",
        "  tf.global_variables_initializer().run()\n",
        "  fig,ax = plt.subplots(1,1)\n",
        "  ax.set_xlabel('Epoochs')\n",
        "  ax.set_ylabel('SoftMax Cross Entropy loss')\n",
        "  xs,ytrs,ytes = [],[],[]\n",
        "  for epooch in range(train_epooch):\n",
        "    train_avg_err = 0\n",
        "    test_avg_err = 0\n",
        "    total_batch = int(mnist.train.num_examples/batch_size)\n",
        "\n",
        "    #Loop over all batches\n",
        "    for i in range(total_batch):\n",
        "      batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
        "\n",
        "      _,c,w = session.run([optimizer_adam,cost_sgd,weights_sgd], feed_dict = {x:batch_xs,y_:batch_ys})\n",
        "      train_avg_err += c/total_batch\n",
        "      c = session.run(cost_sgd, feed_dict = {x: mnist.test.images, y_:mnist.test.labels})\n",
        "      test_avg_err += c/total_batch\n",
        "    \n",
        "    xs.append(epooch)\n",
        "    ytrs.append(train_avg_err)\n",
        "    ytes.append(test_avg_err)\n",
        "    plot_dynamics(xs,ytrs,ytes,ax,np.arange(1.3,1.8,step=0.04),'input-Sigmoid(512)-Sigmoid(128)-Sigmoid(Output(10))-AdamOptimizer')\n",
        "\n",
        "    if epooch%display_step==0:\n",
        "      print('Epooch: {}, Train_cost: {}, Test_cost: {}'.format(epooch,train_avg_err,test_avg_err))\n",
        "\n",
        "  #Plot final results\n",
        "  plot_dynamics(xs,ytrs,ytes,ax,np.arange(1.3,1.8,step=0.04),'input-Sigmoid(512)-Sigmoid(128)-Sigmoid(Output(10))-AdamOptimizer')\n",
        "\n",
        "  #Final Accuracy\n",
        "  correct_prediction = tf.equal(tf.argmax(y_sgd,1),tf.argmax(y_,1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
        "  print(\"Accuracy-----> \",accuracy.eval({x:mnist.test.images,y_:mnist.test.labels}))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHC0Skx1H1jE",
        "colab_type": "text"
      },
      "source": [
        "## Model1 + GradientDescentOptimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldMrBue7-YIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as session:\n",
        "  tf.global_variables_initializer().run()\n",
        "  fig,ax = plt.subplots(1,1)\n",
        "  ax.set_xlabel('Epoochs')\n",
        "  ax.set_ylabel('SoftMax Cross Entropy loss')\n",
        "  xs,ytrs,ytes = [],[],[]\n",
        "  for epooch in range(train_epooch):\n",
        "    train_avg_err = 0\n",
        "    test_avg_err = 0\n",
        "    total_batch = int(mnist.train.num_examples/batch_size)\n",
        "\n",
        "    #Loop over all batches\n",
        "    for i in range(total_batch):\n",
        "      batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
        "\n",
        "      _,c,w = session.run([optimizer_sgdc,cost_sgd,weights_sgd], feed_dict = {x:batch_xs,y_:batch_ys})\n",
        "      train_avg_err += c/total_batch\n",
        "      c = session.run(cost_sgd, feed_dict = {x: mnist.test.images, y_:mnist.test.labels})\n",
        "      test_avg_err += c/total_batch\n",
        "    \n",
        "    xs.append(epooch)\n",
        "    ytrs.append(train_avg_err)\n",
        "    ytes.append(test_avg_err)\n",
        "    plot_dynamics(xs,ytrs,ytes,ax,np.arange(1.3,1.8,step=0.04),'input-Sigmoid(512)-Sigmoid(128)-Sigmoid(Output(10))-GradientDescentOptimizer')\n",
        "\n",
        "    if epooch%display_step==0:\n",
        "      print('Epooch: {}, Train_cost: {}, Test_cost: {}'.format(epooch,train_avg_err,test_avg_err))\n",
        "\n",
        "  #Plot final results\n",
        "  plot_dynamics(xs,ytrs,ytes,ax,np.arange(1.3,1.8,step=0.04),'input-Sigmoid(512)-Sigmoid(128)-Sigmoid(Output(10))-GradientDescentOptimizer')\n",
        "\n",
        "  #Final Accuracy\n",
        "  correct_prediction = tf.equal(tf.argmax(y_sgd,1),tf.argmax(y_,1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
        "  print(\"Accuracy-----> \",accuracy.eval({x:mnist.test.images,y_:mnist.test.labels})) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSOKwWXoIK4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_St8FiUOF90G",
        "colab_type": "text"
      },
      "source": [
        "# Model2: input(784)-ReLu(512)-ReLu(128)-Softmax(10)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwYc0hcYGR8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create Model\n",
        "\n",
        "def multilayer_perceptron_relu(x,weights,biases):\n",
        "  print('X: ',x.get_shape(),'W[h1] : ',weights['h1'].get_shape(),'b[h1]: ',biases['b1'].get_shape())\n",
        "\n",
        "  #Hidden layer with Relu activation\n",
        "  layer_1 = tf.add(tf.matmul(x,weights['h1']),biases['b1'])\n",
        "  layer_1 = tf.nn.relu(layer_1)\n",
        "  print('layer_1: ',layer_1.get_shape(),'W[h2] : ',weights['h2'].get_shape(),'b[h2]: ',biases['b2'].get_shape())\n",
        "\n",
        "  #Hidden layer with Relu activation\n",
        "  layer_2 = tf.add(tf.matmul(layer_1,weights['h2']),biases['b2'])\n",
        "  layer_2 = tf.nn.relu(layer_2)\n",
        "  print('layer_2: ',layer_2.get_shape(),'W[out] : ',weights['out'].get_shape(),'b[out]: ',biases['out'].get_shape())\n",
        "  \n",
        "  #Output layer with sigmoid activation\n",
        "  out_layer = tf.add(tf.matmul(layer_2,weights['out']),biases['out'])\n",
        "  out_layer = tf.nn.sigmoid(out_layer)\n",
        "  print('out_layer: ',out_layer.get_shape())\n",
        "\n",
        "  return out_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFbqPwCsG0Pl",
        "colab_type": "text"
      },
      "source": [
        "## Model2 + AdamOptimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsg2itzFGlwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_relu = multilayer_perceptron_relu(x,weights_sgd,biases)\n",
        "\n",
        "cost_relu = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_relu, labels=y_))\n",
        "\n",
        "optimizer_adam = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_relu)\n",
        "optimizer_sgdc = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost_relu)\n",
        "\n",
        "with tf.Session() as session:\n",
        "  tf.global_variables_initializer().run()\n",
        "  fig,ax = plt.subplots(1,1)\n",
        "  ax.set_xlabel('Epoochs')\n",
        "  ax.set_ylabel('SoftMax Cross Entropy loss')\n",
        "  xs,ytrs,ytes = [],[],[]\n",
        "  for epooch in range(train_epooch):\n",
        "    train_avg_err = 0\n",
        "    test_avg_err = 0\n",
        "    total_batch = int(mnist.train.num_examples/batch_size)\n",
        "\n",
        "    #Loop over all batches\n",
        "    for i in range(total_batch):\n",
        "      batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
        "\n",
        "      _,c,w = session.run([optimizer_adam,cost_relu,weights_sgd], feed_dict = {x:batch_xs,y_:batch_ys})\n",
        "      train_avg_err += c/total_batch\n",
        "      c = session.run(cost_relu, feed_dict = {x: mnist.test.images, y_:mnist.test.labels})\n",
        "      test_avg_err += c/total_batch\n",
        "    \n",
        "    xs.append(epooch)\n",
        "    ytrs.append(train_avg_err)\n",
        "    ytes.append(test_avg_err)\n",
        "    plot_dynamics(xs,ytrs,ytes,ax,np.arange(1.3,1.8,step=0.04),'input-ReLu(512)-ReLu(128)-Sigmoid(Output(10))-AdamOptimizer')\n",
        "\n",
        "    if epooch%display_step==0:\n",
        "      print('Epooch: {}, Train_cost: {}, Test_cost: {}'.format(epooch,train_avg_err,test_avg_err))\n",
        "\n",
        "  #Plot final results\n",
        "  plot_dynamics(xs,ytrs,ytes,ax,np.arange(1.3,1.8,step=0.04),'input-ReLu(512)-ReLu(128)-Sigmoid(Output(10))-AdamOptimizer')\n",
        "\n",
        "  #Final Accuracy\n",
        "  correct_prediction = tf.equal(tf.argmax(y_relu,1),tf.argmax(y_,1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
        "  print(\"Accuracy-----> \",accuracy.eval({x:mnist.test.images,y_:mnist.test.labels}))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRpbmzmtIrth",
        "colab_type": "text"
      },
      "source": [
        "## Model2 + GradientDescentOptimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LO_h_zFIMYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as session:\n",
        "  tf.global_variables_initializer().run()\n",
        "  fig,ax = plt.subplots(1,1)\n",
        "  ax.set_xlabel('Epoochs')\n",
        "  ax.set_ylabel('SoftMax Cross Entropy loss')\n",
        "  xs,ytrs,ytes = [],[],[]\n",
        "  for epooch in range(train_epooch):\n",
        "    train_avg_err = 0\n",
        "    test_avg_err = 0\n",
        "    total_batch = int(mnist.train.num_examples/batch_size)\n",
        "\n",
        "    #Loop over all batches\n",
        "    for i in range(total_batch):\n",
        "      batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
        "\n",
        "      _,c,w = session.run([optimizer_sgdc,cost_relu,weights_sgd], feed_dict = {x:batch_xs,y_:batch_ys})\n",
        "      train_avg_err += c/total_batch\n",
        "      c = session.run(cost_relu, feed_dict = {x: mnist.test.images, y_:mnist.test.labels})\n",
        "      test_avg_err += c/total_batch\n",
        "    \n",
        "    xs.append(epooch)\n",
        "    ytrs.append(train_avg_err)\n",
        "    ytes.append(test_avg_err)\n",
        "    plot_dynamics(xs,ytrs,ytes,ax,np.arange(1.3,1.8,step=0.04),'input-ReLu(512)-ReLu(128)-Sigmoid(Output(10))-GradientDescentOptimizer')\n",
        "\n",
        "    if epooch%display_step==0:\n",
        "      print('Epooch: {}, Train_cost: {}, Test_cost: {}'.format(epooch,train_avg_err,test_avg_err))\n",
        "\n",
        "  #Plot final results\n",
        "  plot_dynamics(xs,ytrs,ytes,ax,np.arange(1.3,1.8,step=0.04),'input-ReLu(512)-ReLu(128)-Sigmoid(Output(10))-GradientDescentOptimizer')\n",
        "\n",
        "  #Final Accuracy\n",
        "  correct_prediction = tf.equal(tf.argmax(y_relu,1),tf.argmax(y_,1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
        "  print(\"Accuracy-----> \",accuracy.eval({x:mnist.test.images,y_:mnist.test.labels}))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s1uaZeFK_yj",
        "colab_type": "text"
      },
      "source": [
        "# Model3 : input(784)-ReLu(BatchNormalization(512))-ReLu(BatchNormalization(128))-Sigmoid(output(10))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC7pbGFMLYP1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create Model\n",
        "#batch normalization always occur on zi's not on xi's\n",
        "\n",
        "epsilon = 1e-3\n",
        "def multilayer_perceptron_batch_norm(x,weights,biases):\n",
        "  print('X: ',x.get_shape(),'W[h1] : ',weights['h1'].get_shape(),'b[h1]: ',biases['b1'].get_shape())\n",
        "\n",
        "  #Hidden layer with Relu activation and Batch Normalization\n",
        "  layer_1 = tf.add(tf.matmul(x,weights['h1']),biases['b1'])\n",
        "\n",
        "  #Caluclate the mean and the variance\n",
        "  batch_mean_1,batch_variance_1 = tf.nn.moments(layer_1,[0])\n",
        "  \n",
        "  scale_1 = tf.Variable(np.ones([n_hidden_1]))\n",
        "  beta_1 = tf.Variable(np.zeros([n_hidden_1]))\n",
        "  scale_1 = tf.cast(scale_1,tf.float32)\n",
        "  beta_1 = tf.cast(beta_1,tf.float32)\n",
        "  \n",
        "  #Batch Normalization\n",
        "  layer_1 = tf.nn.batch_normalization(layer_1,batch_mean_1,batch_variance_1,beta_1,scale_1,epsilon)\n",
        "  \n",
        "  layer_1 = tf.nn.relu(layer_1)\n",
        "  print('layer_1: ',layer_1.get_shape(),'W[h2] : ',weights['h2'].get_shape(),'b[h2]: ',biases['b2'].get_shape())\n",
        "\n",
        "  #Hidden layer with Relu activation and bacth normalization\n",
        "  layer_2 = tf.add(tf.matmul(layer_1,weights['h2']),biases['b2'])\n",
        "  \n",
        "  #Caluclate the mean and the variance\n",
        "  batch_mean_2,batch_variance_2 = tf.nn.moments(layer_2,[0])\n",
        "  \n",
        "  scale_2 = tf.Variable(np.ones([n_hidden_2]))\n",
        "  beta_2 = tf.Variable(np.zeros([n_hidden_2]))\n",
        "  scale_2 = tf.cast(scale_2,tf.float32)\n",
        "  beta_2 = tf.cast(beta_2,tf.float32)\n",
        "  #Batch Normalization\n",
        "  layer_2 = tf.nn.batch_normalization(layer_2,batch_mean_2,batch_variance_2,beta_2,scale_2,epsilon)\n",
        "  \n",
        "  \n",
        "  layer_2 = tf.nn.relu(layer_2)\n",
        "  print('layer_2: ',layer_2.get_shape(),'W[out] : ',weights['out'].get_shape(),'b[out]: ',biases['out'].get_shape())\n",
        "  \n",
        "  #Output layer with sigmoid activation\n",
        "  out_layer = tf.add(tf.matmul(layer_2,weights['out']),biases['out'])\n",
        "  out_layer = tf.nn.sigmoid(out_layer)\n",
        "  print('out_layer: ',out_layer.get_shape())\n",
        "\n",
        "  return out_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT-3a3a5WOzA",
        "colab_type": "text"
      },
      "source": [
        "## Model3 + AdamOptimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx6DQydMWOOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_batch = multilayer_perceptron_batch_norm(x,weights_sgd,biases)\n",
        "\n",
        "cost_batch = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_batch, labels=y_))\n",
        "\n",
        "optimizer_adam = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_batch)\n",
        "optimizer_sgdc = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost_batch)\n",
        "\n",
        "with tf.Session() as session:\n",
        "  tf.global_variables_initializer().run()\n",
        "  fig,ax = plt.subplots(1,1)\n",
        "  ax.set_xlabel('Epoochs')\n",
        "  ax.set_ylabel('SoftMax Cross Entropy loss')\n",
        "  xs,ytrs,ytes = [],[],[]\n",
        "  for epooch in range(train_epooch):\n",
        "    train_avg_err = 0\n",
        "    test_avg_err = 0\n",
        "    total_batch = int(mnist.train.num_examples/batch_size)\n",
        "\n",
        "    #Loop over all batches\n",
        "    for i in range(total_batch):\n",
        "      batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
        "\n",
        "      _,c,w = session.run([optimizer_adam,cost_batch,weights_sgd], feed_dict = {x:batch_xs,y_:batch_ys})\n",
        "      train_avg_err += c/total_batch\n",
        "      c = session.run(cost_batch, feed_dict = {x: mnist.test.images, y_:mnist.test.labels})\n",
        "      test_avg_err += c/total_batch\n",
        "    \n",
        "    xs.append(epooch)\n",
        "    ytrs.append(train_avg_err)\n",
        "    ytes.append(test_avg_err)\n",
        "    plot_dynamics(xs,ytrs,ytes,ax,np.arange(1.3,1.8,step=0.04),'input-Relu_BatchNorm(512)-ReLu_BatchNorm(128)-Sigmoid(Output(10))-AdamOptimizer')\n",
        "\n",
        "    if epooch%display_step==0:\n",
        "      print('Epooch: {}, Train_cost: {}, Test_cost: {}'.format(epooch,train_avg_err,test_avg_err))\n",
        "\n",
        "  #Plot final results\n",
        "  plot_dynamics(xs,ytrs,ytes,ax,np.arange(1.3,1.8,step=0.04),'input-ReLu_BatchNorm(512)-ReLu_BatchNorm(128)-Sigmoid(Output(10))-AdamOptimizer')\n",
        "\n",
        "  #Final Accuracy\n",
        "  correct_prediction = tf.equal(tf.argmax(y_batch,1),tf.argmax(y_,1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
        "  print(\"Accuracy-----> \",accuracy.eval({x:mnist.test.images,y_:mnist.test.labels}))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpAp_qTXXR05",
        "colab_type": "text"
      },
      "source": [
        "## Model3 + GradientDescentOptimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHrFqaIhXTnj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as session:\n",
        "  tf.global_variables_initializer().run()\n",
        "  fig,ax = plt.subplots(1,1)\n",
        "  ax.set_xlabel('Epoochs')\n",
        "  ax.set_ylabel('SoftMax Cross Entropy loss')\n",
        "  xs,ytrs,ytes = [],[],[]\n",
        "  for epooch in range(train_epooch):\n",
        "    train_avg_err = 0\n",
        "    test_avg_err = 0\n",
        "    total_batch = int(mnist.train.num_examples/batch_size)\n",
        "\n",
        "    #Loop over all batches\n",
        "    for i in range(total_batch):\n",
        "      batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
        "\n",
        "      _,c,w = session.run([optimizer_sgdc,cost_batch,weights_sgd], feed_dict = {x:batch_xs,y_:batch_ys})\n",
        "      train_avg_err += c/total_batch\n",
        "      c = session.run(cost_batch, feed_dict = {x: mnist.test.images, y_:mnist.test.labels})\n",
        "      test_avg_err += c/total_batch\n",
        "    \n",
        "    xs.append(epooch)\n",
        "    ytrs.append(train_avg_err)\n",
        "    ytes.append(test_avg_err)\n",
        "    plot_dynamics(xs,ytrs,ytes,ax,np.arange(1.3,1.8,step=0.04),'input-Relu_BatchNorm(512)-ReLu_BatchNorm(128)-Sigmoid(Output(10))-AdamOptimizer')\n",
        "\n",
        "    if epooch%display_step==0:\n",
        "      print('Epooch: {}, Train_cost: {}, Test_cost: {}'.format(epooch,train_avg_err,test_avg_err))\n",
        "\n",
        "  #Plot final results\n",
        "  plot_dynamics(xs,ytrs,ytes,ax,np.arange(1.3,1.8,step=0.04),'input-ReLu_BatchNorm(512)-ReLu_BatchNorm(128)-Sigmoid(Output(10))-AdamOptimizer')\n",
        "\n",
        "  #Final Accuracy\n",
        "  correct_prediction = tf.equal(tf.argmax(y_batch,1),tf.argmax(y_,1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
        "  print(\"Accuracy-----> \",accuracy.eval({x:mnist.test.images,y_:mnist.test.labels}))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOBmCL-Bbtha",
        "colab_type": "text"
      },
      "source": [
        "# Model4: input(784)-ReLu(512)-DropOut-ReLu(128)-DropOut-Sigmoid(Output(10))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoLqpm4ka2VP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create Model\n",
        "\n",
        "def multilayer_perceptron_dropout(x,weights,biases):\n",
        "  print('X: ',x.get_shape(),'W[h1] : ',weights['h1'].get_shape(),'b[h1]: ',biases['b1'].get_shape())\n",
        "\n",
        "  #Hidden layer with Relu activation\n",
        "  layer_1 = tf.add(tf.matmul(x,weights['h1']),biases['b1'])\n",
        "  layer_1 = tf.nn.relu(layer_1)\n",
        "\n",
        "  #Adding dropout Layer after the 1st hidden layer\n",
        "  layer_1_drop = tf.nn.dropout(layer_1,keep_prob)\n",
        "\n",
        "  print('layer_1: ',layer_1.get_shape(),'W[h2] : ',weights['h2'].get_shape(),'b[h2]: ',biases['b2'].get_shape())\n",
        "\n",
        "  #Hidden layer with Relu activation\n",
        "  layer_2 = tf.add(tf.matmul(layer_1_drop,weights['h2']),biases['b2'])\n",
        "  layer_2 = tf.nn.relu(layer_2)\n",
        "\n",
        "  #Adding dropout Layer after the 1st hidden layer\n",
        "  layer_2_drop = tf.nn.dropout(layer_2,keep_prob)\n",
        "\n",
        "  print('layer_2: ',layer_2.get_shape(),'W[out] : ',weights['out'].get_shape(),'b[out]: ',biases['out'].get_shape())\n",
        "  \n",
        "  #Output layer with sigmoid activation\n",
        "  out_layer = tf.add(tf.matmul(layer_2_drop,weights['out']),biases['out'])\n",
        "  out_layer = tf.nn.sigmoid(out_layer)\n",
        "  print('out_layer: ',out_layer.get_shape())\n",
        "\n",
        "  return out_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-V2xJOcjrSi",
        "colab_type": "text"
      },
      "source": [
        "## Model4 + AdamOptimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-S_sfO7bkmF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_dropout = multilayer_perceptron_dropout(x,weights_sgd,biases)\n",
        "\n",
        "cost_dropout = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_dropout, labels=y_))\n",
        "\n",
        "optimizer_adam = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_dropout)\n",
        "optimizer_sgdc = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost_dropout)\n",
        "\n",
        "with tf.Session() as session:\n",
        "  tf.global_variables_initializer().run()\n",
        "  fig,ax = plt.subplots(1,1)\n",
        "  ax.set_xlabel('Epoochs')\n",
        "  ax.set_ylabel('SoftMax Cross Entropy loss')\n",
        "  xs,ytrs,ytes = [],[],[]\n",
        "  for epooch in range(train_epooch):\n",
        "    train_avg_err = 0\n",
        "    test_avg_err = 0\n",
        "    total_batch = int(mnist.train.num_examples/batch_size)\n",
        "\n",
        "    #Loop over all batches\n",
        "    for i in range(total_batch):\n",
        "      batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
        "\n",
        "      _,c,w = session.run([optimizer_adam,cost_dropout,weights_relu], feed_dict = {x:batch_xs,y_:batch_ys,keep_prob:0.5})\n",
        "      train_avg_err += c/total_batch\n",
        "      c = session.run(cost_dropout, feed_dict = {x: mnist.test.images, y_:mnist.test.labels,keep_prob:1.0})\n",
        "      test_avg_err += c/total_batch\n",
        "    \n",
        "    xs.append(epooch)\n",
        "    ytrs.append(train_avg_err)\n",
        "    ytes.append(test_avg_err)\n",
        "    plot_dynamics(xs,ytrs,ytes,ax,np.arange(1.3,1.8,step=0.04),'input-ReLu(512)-ReLu(128)-Sigmoid(Output(10))-AdamOptimizer')\n",
        "\n",
        "    if epooch%display_step==0:\n",
        "      print('Epooch: {}, Train_cost: {}, Test_cost: {}'.format(epooch,train_avg_err,test_avg_err))\n",
        "\n",
        "  #Plot final results\n",
        "  plot_dynamics(xs,ytrs,ytes,ax,np.arange(1.3,1.8,step=0.04),'input-ReLu(512)-ReLu(128)-Sigmoid(Output(10))-AdamOptimizer')\n",
        "\n",
        "  #Final Accuracy\n",
        "  correct_prediction = tf.equal(tf.argmax(y_dropout,1),tf.argmax(y_,1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
        "  print(\"Accuracy-----> \",accuracy.eval({x:mnist.test.images,y_:mnist.test.labels,keep_prob:1.0}))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46XZA-CGkN1x",
        "colab_type": "text"
      },
      "source": [
        "## Model4 + GradientDescentOptimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7NdDkm7kSpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as session:\n",
        "  tf.global_variables_initializer().run()\n",
        "  fig,ax = plt.subplots(1,1)\n",
        "  ax.set_xlabel('Epoochs')\n",
        "  ax.set_ylabel('SoftMax Cross Entropy loss')\n",
        "  xs,ytrs,ytes = [],[],[]\n",
        "  for epooch in range(train_epooch):\n",
        "    train_avg_err = 0\n",
        "    test_avg_err = 0\n",
        "    total_batch = int(mnist.train.num_examples/batch_size)\n",
        "\n",
        "    #Loop over all batches\n",
        "    for i in range(total_batch):\n",
        "      batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
        "\n",
        "      _,c,w = session.run([optimizer_sgdc,cost_dropout,weights_sgd], feed_dict = {x:batch_xs,y_:batch_ys,keep_prob:0.8})\n",
        "      train_avg_err += c/total_batch\n",
        "      c = session.run(cost_dropout, feed_dict = {x: mnist.test.images, y_:mnist.test.labels,keep_prob:1.0})\n",
        "      test_avg_err += c/total_batch\n",
        "    \n",
        "    xs.append(epooch)\n",
        "    ytrs.append(train_avg_err)\n",
        "    ytes.append(test_avg_err)\n",
        "    plot_dynamics(xs,ytrs,ytes,ax,np.arange(1.3,1.8,step=0.04),'input-ReLu(512)-ReLu(128)-Sigmoid(Output(10))-AdamOptimizer')\n",
        "\n",
        "    if epooch%display_step==0:\n",
        "      print('Epooch: {}, Train_cost: {}, Test_cost: {}'.format(epooch,train_avg_err,test_avg_err))\n",
        "\n",
        "  #Plot final results\n",
        "  plot_dynamics(xs,ytrs,ytes,ax,np.arange(1.3,1.8,step=0.04),'input-ReLu(512)-ReLu(128)-Sigmoid(Output(10))-AdamOptimizer')\n",
        "\n",
        "  #Final Accuracy\n",
        "  correct_prediction = tf.equal(tf.argmax(y_dropout,1),tf.argmax(y_,1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
        "  print(\"Accuracy-----> \",accuracy.eval({x:mnist.test.images,y_:mnist.test.labels,keep_prob:1.0}))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbqCISqFnGlX",
        "colab_type": "text"
      },
      "source": [
        "## Model5: input(784)-ReLu_Batch_Norm(1024)-dropout-ReLu_Batch_Norm(512)-dropout-ReLu_Batch_Norm(128)-dropout-sigmoid(output(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4atPfo5nGbI",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}